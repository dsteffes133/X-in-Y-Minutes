{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet'\n",
    "\n",
    "train_df = {}\n",
    "\n",
    "for partition_id in range(10):\n",
    "    \n",
    "    file_path = f'{base_path}/partition_id={partition_id}/part-0.parquet'\n",
    "    train_df[partition_id] = pd.read_parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for partition_id in range(10):\n",
    "    train_df[partition_id] = train_df[partition_id].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just now realized we need a sample of a singular train_df because they are simply too large to run statistical analysis on in this kernel\n",
    "#We have 5.7 million rows of each variable, lets take 50,000, that is about 1%\n",
    "\n",
    "sample_size = 50_000\n",
    "sample_train_df = {}  # Initialize as an empty dictionary\n",
    "\n",
    "# Loop through the first 10 partitions of train_df\n",
    "for partition_id in range(10):\n",
    "    if isinstance(train_df[partition_id], pd.DataFrame):  # Check if it’s a DataFrame\n",
    "        sample_train_df[partition_id] = train_df[partition_id].tail(sample_size)\n",
    "    else:\n",
    "        print(f\"train_df[{partition_id}] is not a DataFrame!\")\n",
    "\n",
    "# Verify the sample from partition 9\n",
    "print(sample_train_df[9].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us examine the responder to see what it might be\n",
    "train_df[9]['responder_6'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (10,6))\n",
    "plt.plot(train_df[9]['time_id'], train_df[9]['responder_6'])\n",
    "plt.title('responder 6 over time')\n",
    "plt.xlabel('time_id')\n",
    "plt.ylabel('responder 6')\n",
    "plt.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok so that was totally not understandable to me, let's plot over date_id\n",
    "\n",
    "plt.figure(figsize = (10,6))\n",
    "plt.plot(train_df[9]['date_id'], train_df[9]['responder_6'])\n",
    "plt.title('responder 6 over dates')\n",
    "plt.xlabel('date_id')\n",
    "plt.ylabel('responder_6')\n",
    "plt.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and that is our first real breakthrough... because it appears to be clustering back towards a mean of 0... it is volatility we are looking for\n",
    "#let us try the ADF test which tests for stationarity, The ADF test evaluates the null hypothesis (H₀): The time series is non-stationary (has a unit root).\n",
    "#ok so we got a highly negative ADF statistic which means if the ADF Statistic is less than the critical value → Reject the null hypothesis (stationary). and we got a p-value of almost 0 which means there is almost a 0% chance of H0 being true with this being result of our sample.\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "result = adfuller(sample_train_df[9]['responder_6'].dropna())\n",
    "print(\"ADF Statistic:\", result[0])\n",
    "print(\"p-value:\", result[1])\n",
    "\n",
    "if result[1] < 0.05:\n",
    "    print(\"Responder_6 is stationary (volatility-like behavior).\")\n",
    "else:\n",
    "    print(\"Responder_6 is non-stationary (unlikely volatility).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok now it appears we have two candidates for responder_6, volatility and log-return\n",
    "#let us make a histogram and if it creates a normal-distribution it could very well be log-returns and not vol\n",
    "# oh wait... there are negative spikes in the plot... that is impossible for volatility\n",
    "\n",
    "import seaborn as sns\n",
    "sns.histplot(sample_train_df[9]['responder_6'], kde=True)\n",
    "plt.title(\"Distribution of Responder_6\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I want to see the distribution of the sample of responder_6\n",
    "#I also want to use .loc to see where the -5 occurence takes place and see around there maybe 100 responder_6 samples\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "sns.histplot(sample_train_df[5]['responder_6'], kde = True)\n",
    "plt.title('Sample histogram plot of responder_6')\n",
    "plt.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok what the heck... let's create a new sample from the head, because how could all of the data samples fit this dist, it doesnt make any sense\n",
    "\n",
    "sample_train_df2 = {}\n",
    "\n",
    "for parameter_id in range(10):\n",
    "\n",
    "    sample_train_df2[partition_id] = train_df[partition_id].head(sample_size)\n",
    "\n",
    "sample_train_df2[9].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new histogram\n",
    "\n",
    "sns.histplot(sample_train_df2[9]['responder_6'], kde=True)\n",
    "plt.title('responder_6 distribution when from second sample')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok this is very interesting... this means that sampling from the head and tail gives you the exact same distribution?? or am i doing something wrong?\n",
    "#i am now going to collect a random sample\n",
    "#The sample will be from 300,000 to 350,000 so 1% of the total df\n",
    "random_sample_train_df = {}\n",
    "\n",
    "for parameter_id in range(10):\n",
    "    random_sample_train_df[parameter_id] = train_df[parameter_id].iloc[300000:350000]\n",
    "\n",
    "random_sample_train_df[9].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.histplot(random_sample_train_df[9]['responder_6'], kde=True)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample_train_df[9]['responder_6'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok... its the same lol, my plan now is to take a super small subsection to see if it is user error\n",
    "\n",
    "final_sample_train_df = {}\n",
    "\n",
    "for parameter_id in range(10):\n",
    "    final_sample_train_df[parameter_id] = train_df[parameter_id].iloc[750000:750100]\n",
    "\n",
    "final_sample_train_df[9].describe()\n",
    "final_sample_train_df[9]['responder_6'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(final_sample_train_df[9]['responder_6'], kde=True)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok so yay, this is real data!\n",
    "#lets create a function to generate a sample dataframe\n",
    "import numpy as np\n",
    "\n",
    "def current_sample_df(train_df, sample_size):\n",
    "    current_sample_df = {}\n",
    "\n",
    "    for parameter_id in range(10):\n",
    "        total_rows = len(train_df[parameter_id])\n",
    "\n",
    "        # Check if sample_size is valid for the current partition\n",
    "        if total_rows >= sample_size:\n",
    "            start_idx = np.random.randint(0, total_rows - sample_size)\n",
    "            current_sample_df[parameter_id] = train_df[parameter_id].iloc[start_idx:start_idx + sample_size]\n",
    "        else:\n",
    "            print(f\"Warning: Partition {parameter_id} has only {total_rows} rows, less than {sample_size}. Skipping.\")\n",
    "            current_sample_df[parameter_id] = None  # Mark as None or handle differently\n",
    "\n",
    "    return current_sample_df\n",
    "\n",
    "print(current_sample_df(train_df, 30000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#created a function to examine the count for values within a dataframe (think it will help me get a grasp on outliers)\n",
    "#There are 1525 -5 values and 8539 +5 values in train_df[9]; that is a 5.6X, so our graph is heavily skewed to the right indicating log-returns imo\n",
    "def count_values(df, column_name, test_value):\n",
    "\n",
    "    count_values = 0\n",
    "\n",
    "    for value in df[column_name]:\n",
    "        if value == test_value:\n",
    "            count_values +=1\n",
    "    return count_values\n",
    "\n",
    "\n",
    "print(count_values(train_df[9], 'responder_6', -5))\n",
    "print(count_values(train_df[9], 'responder_6', 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_acf(sample_train_df[9]['responder_6'], lags=40)\n",
    "plt.ylim(-0.1,0.1)\n",
    "plt.title(\"ACF of Responder_6\")\n",
    "plt.show()\n",
    "\n",
    "plot_acf(sample_train_df[9]['responder_6']**2, lags=40)\n",
    "plt.ylim(-0.1,0.1)\n",
    "plt.title(\"ACF of Squared Responder_6\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "# Test for autocorrelation in raw and squared responder_6\n",
    "lb_raw = acorr_ljungbox(sample_train_df[9]['responder_6'].dropna(), lags=[10, 20, 30], return_df=True)\n",
    "lb_squared = acorr_ljungbox(sample_train_df[9]['responder_6'].dropna()**2, lags=[10, 20, 30], return_df=True)\n",
    "\n",
    "print(\"Ljung-Box Test for Raw Responder_6:\")\n",
    "print(lb_raw)\n",
    "print(\"\\nLjung-Box Test for Squared Responder_6:\")\n",
    "print(lb_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#examination of types of distribution, starting with the laplace distribution.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate Laplace-distributed data\n",
    "mu = np.median(sample_train_df[9]['responder_6'])\n",
    "b = np.sqrt((0.752648**2)/2)\n",
    "\n",
    "data = np.random.laplace(mu, b, len(sample_train_df[9]['responder_6']))\n",
    "\n",
    "# Plot the histogram\n",
    "plt.hist(data, bins=50, density=True, alpha=0.6, color='blue')\n",
    "\n",
    "# Add the theoretical PDF\n",
    "x = np.linspace(-6, 6, 100000)\n",
    "pdf = (1 / (2 * b)) * np.exp(-np.abs(x - mu) / b)\n",
    "plt.plot(x, pdf, 'r', linewidth=2, label=\"Laplace PDF\")\n",
    "\n",
    "plt.title(\"Laplace Distribution\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_1samp, laplace\n",
    "\n",
    "mu = np.median(sample_train_df[9]['responder_6'])\n",
    "b = np.sqrt((0.752648**2) / 2)  # Scale parameter\n",
    "x = sample_train_df[9]['responder_6']\n",
    "\n",
    "# Use the theoretical Laplace CDF\n",
    "laplace_cdf = lambda x: laplace.cdf(x, loc=mu, scale=b)\n",
    "\n",
    "# Perform Kolmogorov-Smirnov test\n",
    "ks_stat, p_value = ks_1samp(x, laplace_cdf)\n",
    "print(f\"KS Statistic: {ks_stat}, p-value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#examination of weight column... it appears that they go in a pattern that has a period of about 40 index units\n",
    "\n",
    "#weight has a high of 5.162702, a low of 0.728737, a mean of 2.101985, and a std dev of 1.001273\n",
    "\n",
    "#The mean is around 2... it is heavily skewed to the right, indicating that the median is most likely less than 2.... it is 2.03 which is less than the mean\n",
    "\n",
    "df = {}\n",
    "\n",
    "for parameter_id in range(10):\n",
    "    df[parameter_id] = sample_train_df[parameter_id].iloc[0:150]\n",
    "\n",
    "df[9].head()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(df[9].index, df[9]['weight'])\n",
    "plt.ylabel('weight')\n",
    "plt.xlabel('time')\n",
    "plt.title('The weight values over time')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotted the weights compared to the responder 6 values... it seems to me that the weight is not very meaningful as \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(df[9].index, df[9]['weight'], df[9]['responder_6'])\n",
    "plt.ylabel('weight')\n",
    "plt.xlabel('time')\n",
    "plt.title('The weight values over time')\n",
    "# Adjust tick density\n",
    "plt.show\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# Plot weight on the primary y-axis\n",
    "ax1.plot(df[9].index, df[9]['weight'], label='Weight', color='blue')\n",
    "ax1.set_ylabel('Weight', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Create a second y-axis for the responder_weight_ratio\n",
    "ax2 = ax1.twinx()  # Instantiate a second y-axis that shares the same x-axis\n",
    "ax2.plot(df[9].index, df[9]['responder_weight_ratio'], label='Responder/Weight Ratio', color='orange')\n",
    "ax2.set_ylabel('Responder/Weight Ratio', color='orange')\n",
    "ax2.tick_params(axis='y', labelcolor='orange')\n",
    "\n",
    "# Add title and show plot\n",
    "plt.title('Weight and Responder/Weight Ratio Over Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final function for running test\n",
    "\n",
    "def predict(test: pd.DataFrame, lags: pd.DataFrame | None) -> pl.DataFrame | pd.DataFrame:\n",
    "    \"\"\"Make a prediction.\"\"\"\n",
    "    # All the responders from the previous day are passed in at time_id == 0. We save them in a global variable for access at every time_id.\n",
    "    # Use them as extra features, if you like.\n",
    "    global lags_\n",
    "    if lags is not None:\n",
    "        lags_ = lags\n",
    "\n",
    "    # Replace this section with your own predictions\n",
    "    predictions = test.select(\n",
    "        'row_id',\n",
    "        pl.lit(0.0).alias('responder_6'),\n",
    "    )\n",
    "\n",
    "    \n",
    "    if isinstance(predictions, pd.DataFrame):\n",
    "        assert (predictions.columns == ['row_id', 'responder_6']).all()\n",
    "    else:\n",
    "        raise TypeError('The predict function must return a DataFrame')\n",
    "    # Confirm has as many rows as the test data.\n",
    "    assert len(predictions) == len(test)\n",
    "\n",
    "    return predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
